{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL to SCRAP\n",
    "HOST_URL = \"https://172.31.1.62/\"\n",
    "\n",
    "# The name of the Sequencing Run you are importing\n",
    "SEQUENCING_RUN_NAME = \"TKMT01\"\n",
    "\n",
    "S3_BUCKET = \"scrap-dm\"\n",
    "\n",
    "# The existing S3 path to the sequencing run directory\n",
    "SEQUENCING_RUN_DIRECTORY = \"reads/210910_A01102_0279_AHM2KKDSX2_nomismatches/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for FASTQ files in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = os.popen(\"aws s3 ls --recursive %s/%s\" % (S3_BUCKET, SEQUENCING_RUN_DIRECTORY)).read() # list all files found in an S3 folder\n",
    "source_files = source_files.split('\\n') # create a list from string output\n",
    "source_files = [x.split(' ')[-1] for x in source_files if x.endswith('.fastq.gz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_names = set()\n",
    "\n",
    "# Loop through all the detected FASTQs and extract the unique sample names\n",
    "for file in source_files:\n",
    "    prefix, file_name = os.path.split(file)\n",
    "    \n",
    "    sample_name = \"_\".join(file_name.split(\"_\")[0:-4])\n",
    "    new_sample_names.add(sample_name)\n",
    "\n",
    "# We don't want to save the undetermineds, so we remove them\n",
    "new_sample_names.remove(\"Undetermined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create samples in SCRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all existing Cell Sets, just to make sure they don't exist already\n",
    "cell_sets_URL = HOST_URL + \"cell_sets\"\n",
    "\n",
    "response = requests.get(cell_sets_URL, verify=False)\n",
    "cell_sets = response.json()[\"cell_sets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_sets_URL = HOST_URL + \"cell_sets\"\n",
    "\n",
    "for cell_set_name in new_sample_names:\n",
    "    \n",
    "    found_cell_set = False\n",
    "    \n",
    "    for cell_set in cell_sets:\n",
    "        if cell_set[\"name\"] == cell_set_name:\n",
    "            print(\"%s already exists -- using\" % cell_set_name)\n",
    "            found_cell_set = True\n",
    "            break\n",
    "            \n",
    "    if found_cell_set:\n",
    "        continue\n",
    "        \n",
    "    print(\"Creating cell set %s\" % cell_set_name)\n",
    "    \n",
    "    cell_set = {\n",
    "        \"name\": cell_set_name,\n",
    "        \"status\": \"available\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(cell_sets_URL, json=cell_set, verify=False)\n",
    "    cell_set = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sequencing run associated with these samples, or create it if it doesn't exist\n",
    "sequencing_runs_URL = HOST_URL + \"sequencing_runs\"\n",
    "\n",
    "response = requests.get(sequencing_runs_URL, verify=False)\n",
    "sequencing_runs = response.json()[\"sequencing_runs\"]\n",
    "\n",
    "sequencing_run_id = None\n",
    "\n",
    "for sequencing_run in sequencing_runs:\n",
    "    if sequencing_run[\"name\"] == SEQUENCING_RUN_NAME:\n",
    "        sequencing_run_id = sequencing_run[\"_id\"]\n",
    "        print(\"Found sequencing run %s\" % SEQUENCING_RUN_NAME)\n",
    "        break\n",
    "\n",
    "if not sequencing_run_id:\n",
    "    sequencing_run = {}\n",
    "    sequencing_run[\"type\"] = \"local_BCLs\"\n",
    "    sequencing_run[\"name\"] = SEQUENCING_RUN_NAME\n",
    "    sequencing_run[\"status\"] = \"available\"\n",
    "    sequencing_run[\"cell_sets\"] = []\n",
    "    \n",
    "    print(\"Creating sequencing run %s\" % SEQUENCING_RUN_NAME)\n",
    "    response = requests.post(sequencing_runs_URL, json=sequencing_run, verify=False)\n",
    "    sequencing_run = response.json()\n",
    "    sequencing_run_id = response.json()[\"_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_sets_URL = HOST_URL + \"read_sets\"\n",
    "response = requests.get(read_sets_URL, verify=False)\n",
    "read_sets = response.json()[\"read_sets\"]\n",
    "\n",
    "response = requests.get(cell_sets_URL, verify=False)\n",
    "cell_sets = response.json()[\"cell_sets\"]\n",
    "sample_cell_sets = {}\n",
    "\n",
    "for sample_name in new_sample_names:\n",
    "    \n",
    "    for cell_set in cell_sets:\n",
    "        \n",
    "        if cell_set[\"name\"] == sample_name:\n",
    "            sample_cell_sets[sample_name] = cell_set\n",
    "            break\n",
    "\n",
    "for sample_index, sample_name in enumerate(new_sample_names):\n",
    "    \n",
    "    read_set_id = None\n",
    "    \n",
    "    for read_set in read_sets:\n",
    "        if read_set[\"name\"] == sample_name:\n",
    "            read_set_id = read_set[\"_id\"]\n",
    "            print(\"Found existing read set %s\" % sample_name)\n",
    "            break\n",
    "\n",
    "    if not read_set_id:\n",
    "        read_set = {}\n",
    "        read_set[\"name\"] = sample_name\n",
    "        read_set[\"status\"] = \"available\"\n",
    "        \n",
    "        print(\"Creating read set %s\" % sample_name)\n",
    "        \n",
    "        response = requests.post(read_sets_URL, json=read_set, verify=False)\n",
    "        read_set = response.json()\n",
    "        read_set[\"sequencing_run_id\"] = sequencing_run_id\n",
    "        read_set[\"status\"] = \"available\"\n",
    "        read_set[\"cell_set_id\"] = sample_cell_sets[sample_name][\"_id\"]\n",
    "        \n",
    "        response = requests.put(read_sets_URL + \"/%s\" % read_set[\"_id\"], json=read_set, verify=False)\n",
    "        read_set = response.json()\n",
    "    \n",
    "    print(read_set[\"path_id\"])\n",
    "    \n",
    "    destination_files = os.popen('aws s3 ls s3://scrap-dm/reads/%s/' % read_set[\"path_id\"]).read() # list all files found in an S3 folder\n",
    "    destination_files = destination_files.split('\\n') # create a list from string output\n",
    "    destination_files = [x.split(' ')[-1] for x in destination_files if x.endswith('.fastq.gz')] # only get .gz elements and file names\n",
    "    \n",
    "    sample_source_files = []\n",
    "    \n",
    "    for file_name in source_files:\n",
    "        file_sample_name = os.path.split(file_name)[1].split(\"_\")\n",
    "        file_sample_name = \"_\".join(file_sample_name[0:-4])\n",
    "        if file_sample_name == sample_name:\n",
    "            sample_source_files.append(file_name)\n",
    "            \n",
    "    print(\"Source files:\")\n",
    "    print(sample_source_files)\n",
    "    print(\"Destination files:\")\n",
    "    print(destination_files)\n",
    "    \n",
    "    for source_file in sample_source_files:\n",
    "        \n",
    "        file_name = os.path.split(source_file)[1]\n",
    "        new_file_name = file_name\n",
    "        \n",
    "        while new_file_name in destination_files:\n",
    "            new_file_name_parts = new_file_name[0:-9].split(\"_\")\n",
    "            new_file_name = \"_\".join(new_file_name_parts[0:-1]) + \"_%03d\" % (int(new_file_name_parts[-1])+1) + \".fastq.gz\"\n",
    "        \n",
    "        if new_file_name != file_name:\n",
    "            print(\"Will rename %s to %s\" % (file_name, new_file_name))\n",
    "            \n",
    "        source_path = \"s3://%s/%s\" % (S3_BUCKET, source_file)\n",
    "        destination_path = \"s3://scrap-dm/reads/%s/%s\" % (read_set[\"path_id\"], new_file_name)\n",
    "        print(\"Moving %s to %s\" % (source_path, destination_path))\n",
    "        os.system('aws s3 mv %s %s' % (source_path, destination_path))\n",
    "        read_set[\"FASTQ_files\"][new_file_name] = {\"remote_path\" : new_file_name, \"status\" : \"available\"}\n",
    "        response = requests.put(read_sets_URL + \"/%s\" % read_set[\"_id\"], json=read_set, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"aws s3 mv --recursive s3://%s/%s s3://%s/sequencing_runs/%s/\" % (S3_BUCKET, SEQUENCING_RUN_DIRECTORY, S3_BUCKET, sequencing_run[\"path_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
